{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOSydsZhgTFhxzmPN5SfziT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing\n",
        "- 클렌징 (Cleansing)\n",
        "- 토큰화 (Tokenization)\n",
        "- 필터링 (Filtering) / Stop word 제거 / 철자 수정\n",
        "- 어간 추출 (Stemming)\n",
        "- 표제어 추출 (Lemmatization)"
      ],
      "metadata": {
        "id": "hxTdl5JpYIXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 토큰화 (Tokenization)"
      ],
      "metadata": {
        "id": "x-R7yylKYNP9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE_n6A7JYDwm",
        "outputId": "7cf849c7-1100-49d4-bd7b-39d1b00f917e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "sentence tokenization\n",
            "3\n",
            "['The matrix is everywhere its all around us, here even in my room.', 'You can see it out your window or on your tv.', 'you feel it when you go to work, or go to church or pay your taxes.']\n",
            "==================================================\n",
            "words tokenization : \n",
            "45\n",
            "['The', 'matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'my', 'room', '.', 'You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'tv', '.', 'you', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "from nltk import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text_sample = 'The matrix is everywhere its all around us, here even in my room. You can see it out your window or on your tv. you feel it when you go to work, or go to church or pay your taxes.'\n",
        "sentences = sent_tokenize(text =text_sample)\n",
        "print('sentence tokenization' )\n",
        "print(len(sentences))\n",
        "print (sentences);print('='*50)\n",
        "\n",
        "words = word_tokenize(text= text_sample)\n",
        "print('words tokenization : ')\n",
        "print(len(words))\n",
        "print(words);print('='*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    sentences = sent_tokenize(text=text)\n",
        "    words = [word_tokenize(_) for _ in sentences]\n",
        "    return words\n",
        "\n",
        "result = tokenize_text(text=text_sample)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42saqY7PYdh-",
        "outputId": "465d2c02-08f0-4ac0-80d6-923fd16f80e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['The', 'matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'my', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'tv', '.'], ['you', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Stop word 제거\n"
      ],
      "metadata": {
        "id": "_uAQvxygbd5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens=[]\n",
        "\n",
        "for sentence in result :\n",
        "    filtered_words=[]\n",
        "    for word in sentence :\n",
        "        word = word.lower()\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    all_tokens.append(filtered_words)\n",
        "\n",
        "print(all_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iakCGjA2bkyk",
        "outputId": "72a4077a-085d-48a3-8088-de11245fce04"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'tv', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 어간 추출 (Stemming) 표제어 추출 (Lemmatization)"
      ],
      "metadata": {
        "id": "dfH2lzrbe6oR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "stemmer = LancasterStemmer()\n",
        "lemmer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "words_list = [['working','works','worked'],['amusing','amuses','amused'],['happier','happiest'],['fancier','fanciest']]\n",
        "word_t=['v','v','a','a']\n",
        "for i, words in enumerate(words_list):\n",
        "    stem = []; lemm=[]\n",
        "    for word in words :\n",
        "        stem.append(stemmer.stem(word)) \n",
        "        lemm.append(lemmer.lemmatize(word, word_t[i])) \n",
        "    print('stem : ',stem)\n",
        "    print('lemm : ',lemm)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUFKQ4Bxe_Eg",
        "outputId": "e8d76797-1322-4906-ec95-cec796a185a3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "stem :  ['work', 'work', 'work']\n",
            "lemm :  ['work', 'work', 'work']\n",
            "stem :  ['amus', 'amus', 'amus']\n",
            "lemm :  ['amuse', 'amuse', 'amuse']\n",
            "stem :  ['happy', 'happiest']\n",
            "lemm :  ['happy', 'happy']\n",
            "stem :  ['fant', 'fanciest']\n",
            "lemm :  ['fancy', 'fancy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6eMh8J95fQ07"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}